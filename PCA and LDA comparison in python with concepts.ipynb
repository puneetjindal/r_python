{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length in cm</th>\n",
       "      <th>sepal width in cm</th>\n",
       "      <th>petal length in cm</th>\n",
       "      <th>petal width in cm</th>\n",
       "      <th>class label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length in cm  sepal width in cm  petal length in cm  \\\n",
       "145                 6.7                3.0                 5.2   \n",
       "146                 6.3                2.5                 5.0   \n",
       "147                 6.5                3.0                 5.2   \n",
       "148                 6.2                3.4                 5.4   \n",
       "149                 5.9                3.0                 5.1   \n",
       "\n",
       "     petal width in cm     class label  \n",
       "145                2.3  Iris-virginica  \n",
       "146                1.9  Iris-virginica  \n",
       "147                2.0  Iris-virginica  \n",
       "148                2.3  Iris-virginica  \n",
       "149                1.8  Iris-virginica  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The general LDA approach is very similar to a Principal Component Analysis  \n",
    "but in addition to finding the component axes that maximize the variance of our data (PCA), \n",
    "we are additionally interested in the axes that maximize the separation between multiple classes (LDA).\n",
    "Both Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are linear transformation techniques \n",
    "that are commonly used for dimensionality reduction. PCA can be described as an “unsupervised” algorithm, \n",
    "since it “ignores” class labels and its goal is to find the directions (the so-called principal components) \n",
    "that maximize the variance in a dataset. In contrast to PCA, LDA is “supervised” and computes the \n",
    "directions (“linear discriminants”) that will represent the axes that that maximize the separation between multiple classes.\n",
    "\n",
    "Although it might sound intuitive that LDA is superior to PCA for a multi-class classification task where the \n",
    "class labels are known, this might not always the case.\n",
    "For example, comparisons between classification accuracies for image recognition after using PCA or LDA show that PCA \n",
    "tends to outperform LDA if the number of samples per class is relatively small. In practice, it is also not uncommon\n",
    "to use both LDA and PCA in combination: E.g., PCA for dimensionality reduction followed by an LDA.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    " \n",
    "df = pd.io.parsers.read_csv(\n",
    "    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n",
    "    header=None, \n",
    "    sep=',', \n",
    "    )\n",
    "feature_dict = {i:label for i,label in zip(\n",
    "            range(4),\n",
    "              ('sepal length in cm', \n",
    "              'sepal width in cm', \n",
    "              'petal length in cm', \n",
    "              'petal width in cm', ))}\n",
    "df.columns = [l for i,l in sorted(feature_dict.items())] + ['class label']\n",
    "df.dropna(how=\"all\", inplace=True) # to drop the empty line at file-end\n",
    " \n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    " \n",
    "X = df[[0,1,2,3]].values \n",
    "y = df['class label'].values\n",
    " \n",
    "enc = LabelEncoder()\n",
    "label_encoder = enc.fit(y)\n",
    "y = label_encoder.transform(y) + 1\n",
    " \n",
    "label_dict = {1: 'Setosa', 2: 'Versicolor', 3:'Virginica'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    " \n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,6))\n",
    " \n",
    "for ax,cnt in zip(axes.ravel(), range(4)):\n",
    " \n",
    "    # set bin sizes\n",
    "    min_b = math.floor(np.min(X[:,cnt]))\n",
    "    max_b = math.ceil(np.max(X[:,cnt]))\n",
    "    bins = np.linspace(min_b, max_b, 25)\n",
    " \n",
    "    # plottling the histograms\n",
    "    for lab,col in zip(range(1,4), ('blue', 'red', 'green')):\n",
    "        ax.hist(X[y==lab, cnt],\n",
    "                   color=col, \n",
    "                   label='class %s' %label_dict[lab], \n",
    "                   bins=bins,\n",
    "                   alpha=0.5,)\n",
    "    ylims = ax.get_ylim()\n",
    " \n",
    "    # plot annotation\n",
    "    leg = ax.legend(loc='upper right', fancybox=True, fontsize=8)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    ax.set_ylim([0, max(ylims)+2])\n",
    "    ax.set_xlabel(feature_dict[cnt])\n",
    "    ax.set_title('Iris histogram #%s' %str(cnt+1))\n",
    " \n",
    "    # hide axis ticks\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    " \n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False) \n",
    "    ax.spines[\"bottom\"].set_visible(False) \n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    " \n",
    "axes[0][0].set_ylabel('count')\n",
    "axes[1][0].set_ylabel('count')\n",
    " \n",
    "fig.tight_layout()\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Vector class 1: [-1.0146  0.8423 -1.3049 -1.2551]\n",
      "\n",
      "Mean Vector class 2: [ 0.1123 -0.6572  0.2851  0.1674]\n",
      "\n",
      "Mean Vector class 3: [ 0.9023 -0.1851  1.0198  1.0877]\n",
      "\n",
      "('within-class Scatter Matrix:\\n', array([[ 57.1941,  38.3652,  16.9598,   9.0095],\n",
      "       [ 38.3652,  91.2179,  10.685 ,  14.9475],\n",
      "       [ 16.9598,  10.685 ,   8.8022,   4.6754],\n",
      "       [  9.0095,  14.9475,   4.6754,  10.6746]]))\n",
      "('between-class Scatter Matrix:\\n', array([[ 371.2234, -219.0824,  455.2134,  454.734 ],\n",
      "       [-219.0824,  235.1285, -295.0497, -273.7166],\n",
      "       [ 455.2134, -295.0497,  564.7914,  558.9527],\n",
      "       [ 454.734 , -273.7166,  558.9527,  557.3016]]))\n",
      "\n",
      "Eigenvector 1: \n",
      "[[-0.1498]\n",
      " [-0.1482]\n",
      " [ 0.8511]\n",
      " [ 0.4808]]\n",
      "Eigenvalue 1: 1.29e+02\n",
      "\n",
      "Eigenvector 2: \n",
      "[[ 0.0095]\n",
      " [ 0.3272]\n",
      " [-0.5748]\n",
      " [ 0.75  ]]\n",
      "Eigenvalue 2: 1.11e+00\n",
      "\n",
      "Eigenvector 3: \n",
      "[[-0.7989]\n",
      " [ 0.0507]\n",
      " [ 0.0831]\n",
      " [ 0.5935]]\n",
      "Eigenvalue 3: 3.74e-15\n",
      "\n",
      "Eigenvector 4: \n",
      "[[ 0.3742]\n",
      " [ 0.0876]\n",
      " [ 0.5075]\n",
      " [-0.7712]]\n",
      "Eigenvalue 4: 1.07e-14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIf we are performing the LDA for dimensionality reduction, the eigenvectors are important since they will form the new axes \\nof our new feature subspace; the associated eigenvalues are of particular interest since they will tell us how informative \\nthe new axes are.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "preprocessing.scale(X, axis=0, with_mean=True, with_std=True, copy=False)\n",
    "#step 1 : compute the d dimension mean vector\n",
    "np.set_printoptions(precision=4)\n",
    " \n",
    "mean_vectors = []\n",
    "for cl in range(1,4):\n",
    "    mean_vectors.append(np.mean(X[y==cl], axis=0))\n",
    "    print('Mean Vector class %s: %s\\n' %(cl, mean_vectors[cl-1]))\n",
    "    \n",
    "\n",
    "# Compute the scatter matrices\n",
    "\n",
    "# 1 Sw within class scatter matrix\n",
    "S_W = np.zeros((4,4))\n",
    "for cl,mv in zip(range(1,4), mean_vectors):\n",
    "    class_sc_mat = np.zeros((4,4))                  # scatter matrix for every class\n",
    "    for row in X[y == cl]:\n",
    "        row, mv = row.reshape(4,1), mv.reshape(4,1) # make column vectors\n",
    "        class_sc_mat += (row-mv).dot((row-mv).T)\n",
    "    S_W += class_sc_mat                             # sum class scatter matrices\n",
    "print('within-class Scatter Matrix:\\n', S_W)\n",
    "\n",
    "\n",
    "# 2 Between class matrix\n",
    "overall_mean = np.mean(mean_vectors, axis=0)\n",
    " \n",
    "S_B = np.zeros((4,4))\n",
    "for i,mean_vec in enumerate(mean_vectors):  \n",
    "    n = X[y==i+1,:].shape[0]\n",
    "    mean_vec = mean_vec.reshape(4,1) # make column vector\n",
    "    S_B += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)\n",
    " \n",
    "print('between-class Scatter Matrix:\\n', S_B)\n",
    "\n",
    "\n",
    "#solving the generalized eignevalue problem for the matrix Sw(inverse)Sb\n",
    "eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
    " \n",
    "for i in range(len(eig_vals)):\n",
    "    eigvec_sc = eig_vecs[:,i].reshape(4,1)   \n",
    "    print('\\nEigenvector {}: \\n{}'.format(i+1, eigvec_sc.real))\n",
    "    print('Eigenvalue {:}: {:.2e}'.format(i+1, eig_vals[i].real))\n",
    "    \n",
    "\"\"\"\n",
    "Each of these eigenvectors is associated with an eigenvalue, which tells us about the “length” or “magnitude” of the \n",
    "eigenvectors.\n",
    "If we would observe that all eigenvalues have a similar magnitude, then this may be a good indicator that our \n",
    "data is already projected on a “good” feature space.\n",
    "\n",
    "And in the other scenario, if some of the eigenvalues are much much larger than others, we might be interested in keeping \n",
    "only those eigenvectors with the highest eigenvalues, since they contain more information about our data distribution. \n",
    "Vice versa, eigenvalues that are close to 0 are less informative and we might consider dropping those for constructing \n",
    "the new feature subspace.\n",
    "If we are performing the LDA for dimensionality reduction, the eigenvectors are important since they will form the new axes\n",
    "of our new feature subspace; the associated eigenvalues are of particular interest since they will tell us how informative \n",
    "the new axes are.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# Checking the eigen vector - eigenvalue calculation\n",
    "\n",
    "for i in range(len(eig_vals)):\n",
    "    eigv = eig_vecs[:,i].reshape(4,1) \n",
    "    np.testing.assert_array_almost_equal(np.linalg.inv(S_W).dot(S_B).dot(eigv), \n",
    "                                         eig_vals[i] * eigv, \n",
    "                                         decimal=6, err_msg='', verbose=True)\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues in decreasing order:\n",
      "\n",
      "129.087831199\n",
      "1.11026745536\n",
      "1.06627406467e-14\n",
      "3.73796164842e-15\n"
     ]
    }
   ],
   "source": [
    "# Step 4 selecting linear discriminants for the new feature space\n",
    "# Sort the eigen vectors by decreasing eigenvalues\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    " \n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True)\n",
    " \n",
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    " \n",
    "print('Eigenvalues in decreasing order:\\n')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained:\n",
      "\n",
      "eigenvalue 1: 99.15%\n",
      "eigenvalue 2: 0.85%\n",
      "eigenvalue 3: 0.00%\n",
      "eigenvalue 4: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print('Variance explained:\\n')\n",
    "eigv_sum = sum(eig_vals)\n",
    "for i,j in enumerate(eig_pairs):\n",
    "    print('eigenvalue {0:}: {1:.2%}'.format(i+1, (j[0]/eigv_sum).real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Matrix W:\\n', array([[-0.1498, -0.1482,  0.8511,  0.4808],\n",
      "       [ 0.0095,  0.3272, -0.5748,  0.75  ]]))\n"
     ]
    }
   ],
   "source": [
    "# chosing k eigenvectors with the largest eigenvalues\n",
    "W = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))\n",
    "print('Matrix W:\\n', W.real.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 5 transforming the samples onto new subspace\n",
    "X_lda = W.T.dot(X.T).T\n",
    "assert X_lda.shape == (150,2), \"The matrix is not 2x150 dimensional.\"\n",
    "def plot_step_lda():\n",
    " \n",
    "    ax = plt.subplot(111)\n",
    "    for label,marker,color in zip(\n",
    "        range(1,4),('^', 's', 'o'),('blue', 'red', 'green')):\n",
    " \n",
    "        plt.scatter(x=X_lda[:,0][y == label],\n",
    "                y=X_lda[:,1][y == label],\n",
    "                marker=marker,\n",
    "                color=color,\n",
    "                alpha=0.5,\n",
    "                label=label_dict[label]\n",
    "                )\n",
    " \n",
    "    plt.xlabel('LD1')\n",
    "    plt.ylabel('LD2')\n",
    " \n",
    "    leg = plt.legend(loc='upper right', fancybox=True)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    plt.title('LDA: Iris projection onto the first 2 linear discriminants')\n",
    " \n",
    "    # hide axis ticks\n",
    "    plt.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    " \n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False) \n",
    "    ax.spines[\"bottom\"].set_visible(False) \n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    " \n",
    "    plt.grid()\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    " \n",
    "plot_step_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compare it with PCA\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    " \n",
    "sklearn_pca = sklearnPCA(n_components=2)\n",
    "X_pca = sklearn_pca.fit_transform(X)\n",
    " \n",
    "def plot_pca():\n",
    " \n",
    "    ax = plt.subplot(111)\n",
    " \n",
    "    for label,marker,color in zip(\n",
    "        range(1,4),('^', 's', 'o'),('blue', 'red', 'green')):\n",
    " \n",
    "        plt.scatter(x=X_pca[:,0][y == label],\n",
    "                y=X_pca[:,1][y == label],\n",
    "                marker=marker,\n",
    "                color=color,\n",
    "                alpha=0.5,\n",
    "                label=label_dict[label]\n",
    "                )\n",
    " \n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    " \n",
    "    leg = plt.legend(loc='upper right', fancybox=True)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    plt.title('PCA: Iris projection onto the first 2 principal components')\n",
    " \n",
    "    # hide axis ticks\n",
    "    plt.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    " \n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False) \n",
    "    ax.spines[\"bottom\"].set_visible(False) \n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    " \n",
    "    plt.tight_layout\n",
    "    plt.grid()\n",
    " \n",
    "    plt.show()\n",
    "    \n",
    "plot_pca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LDA from schikit learn library\n",
    "from sklearn.lda import LDA\n",
    " \n",
    "# LDA\n",
    "sklearn_lda = LDA(n_components=2)\n",
    "X_lda_sklearn = sklearn_lda.fit_transform(X, y)\n",
    " \n",
    "# PCA\n",
    "sklearn_pca = sklearnPCA(n_components=2)\n",
    "X_ldapca_sklearn = sklearn_pca.fit_transform(X_lda_sklearn)\n",
    "\n",
    "def plot_scikit_lda(X, title, mirror=1):\n",
    " \n",
    "    ax = plt.subplot(111)\n",
    "    for label,marker,color in zip(\n",
    "        range(1,4),('^', 's', 'o'),('blue', 'red', 'green')):\n",
    " \n",
    "        plt.scatter(x=X[:,0][y == label]*mirror,\n",
    "                y=X[:,1][y == label],\n",
    "                marker=marker,\n",
    "                color=color,\n",
    "                alpha=0.5,\n",
    "                label=label_dict[label]\n",
    "                )\n",
    " \n",
    "    plt.xlabel('LD1')\n",
    "    plt.ylabel('LD2')\n",
    " \n",
    "    leg = plt.legend(loc='upper right', fancybox=True)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    plt.title(title)\n",
    " \n",
    "    # hide axis ticks\n",
    "    plt.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    " \n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False) \n",
    "    ax.spines[\"bottom\"].set_visible(False) \n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    " \n",
    "    plt.grid()\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "    \n",
    "plot_step_lda()\n",
    "plot_scikit_lda(X_ldapca_sklearn, title='LDA+PCA via scikit-learn', mirror=(-1))\n",
    "plot_scikit_lda(X_lda_sklearn, title='Default LDA via scikit-learn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# More can be learnt by looking at http://spartanideas.msu.edu/2014/08/03/linear-discriminant-analysis-bit-by-bit/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
